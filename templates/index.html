<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>NEXUS 3000 • COSMIC AGI</title>
    <style>
        :root {
            --glow-cyan: #00ffff;
            --glow-purple: #8338ec;
            --glow-magenta: #d870f0;
            --glow-blue: #00aaff;
            --dark-bg: #000011;
        }
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        body {
            font-family: 'Courier New', monospace;
            background: var(--dark-bg);
            min-height: 100vh;
            display: flex;
            align-items: center;
            justify-content: center;
            color: var(--glow-cyan);
            overflow: hidden;
            position: relative;
        }
        script { display: none !important; }
        
        /* --- BACKGROUND & PARALLAX LAYERS --- */
        .parallax-layer {
            position: fixed;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            transition: transform 0.2s cubic-bezier(0.25, 0.46, 0.45, 0.94);
        }
        #space-bg {
            background: 
                radial-gradient(ellipse at 20% 30%, #1a0033 0%, transparent 50%), 
                radial-gradient(ellipse at 80% 70%, #0d1421 0%, transparent 50%), 
                radial-gradient(ellipse at 40% 80%, #2d1b69 0%, transparent 60%), 
                radial-gradient(ellipse 1200px 400px at 30% 20%, rgba(0, 212, 255, 0.3) 0%, transparent 40%), 
                radial-gradient(ellipse 800px 300px at 70% 60%, rgba(255, 0, 110, 0.2) 0%, transparent 35%), 
                linear-gradient(125deg, #000011 0%, #0a0a0f 30%, #1a0033 60%, #000022 100%);
            animation: galaxyRotation 240s linear infinite;
            z-index: -4;
        }
        #stars {
            background-image: 
                radial-gradient(2px 2px at 20% 15%, white, transparent),
                radial-gradient(2px 2px at 70% 35%, var(--glow-cyan), transparent),
                radial-gradient(3px 3px at 40% 55%, white, transparent),
                radial-gradient(1px 1px at 80% 75%, var(--glow-magenta), transparent),
                radial-gradient(2px 2px at 10% 85%, var(--glow-purple), transparent);
            background-size: 500px 500px;
            animation: starfield 300s linear infinite, starTwinkle 4s ease-in-out infinite alternate;
            z-index: -3;
        }
        #cosmic-dust {
            background-image: 
                radial-gradient(ellipse 600px 100px at 20% 40%, rgba(0, 212, 255, 0.1) 0%, transparent 70%), 
                radial-gradient(ellipse 800px 80px at 80% 70%, rgba(255, 0, 110, 0.08) 0%, transparent 80%), 
                radial-gradient(ellipse 500px 120px at 60% 20%, rgba(131, 56, 236, 0.07) 0%, transparent 75%);
            animation: cosmicDrift 180s ease-in-out infinite;
            z-index: -2;
        }
        #planets-container { z-index: -1; }
        .planet {
            position: absolute;
            border-radius: 50%;
            box-shadow: 0 0 50px -10px currentColor, inset 0 0 25px -5px rgba(255, 255, 255, 0.4);
        }
        /* (Planet styles remain the same) */
        .planet-1 { width: 180px; height: 180px; color: #ff4848; background: radial-gradient(circle, #c43a3a, #5a1a1a); animation: move-planet-1 80s linear infinite; }
        .planet-2 { width: 80px; height: 80px; color: #48aaff; background: radial-gradient(circle, #3a8bc4, #1a3e5a); animation: move-planet-2 120s linear infinite -30s; }

        @keyframes galaxyRotation { 0% { transform: rotate(0deg) scale(1.2); } 100% { transform: rotate(360deg) scale(1.2); } }
        @keyframes starfield { 0% { background-position: 0 0; } 100% { background-position: -1000px 500px; } }
        @keyframes starTwinkle { 0% { opacity: 0.7; } 100% { opacity: 1; } }
        @keyframes cosmicDrift { 0% { transform: translate(0, 0) rotate(0deg); } 25% { transform: translate(-50px, 25px) rotate(45deg); } 50% { transform: translate(50px, -25px) rotate(90deg); } 75% { transform: translate(-25px, -50px) rotate(135deg); } 100% { transform: translate(0, 0) rotate(180deg); } }
        @keyframes move-planet-1 { 0% { transform: translate(-20vw, 120vh); } 100% { transform: translate(120vw, -20vh); } }
        @keyframes move-planet-2 { 0% { transform: translate(110vw, 80vh); } 100% { transform: translate(-30vw, 10vh); } }

        /* --- UI ELEMENTS --- */
        #start-overlay {
            position: fixed; top: 0; left: 0; width: 100%; height: 100%;
            background: rgba(0, 0, 0, 0.95); display: flex; align-items: center; justify-content: center;
            z-index: 9999; backdrop-filter: blur(10px); cursor: pointer; transition: opacity 0.5s ease;
        }
        #start-button {
            font-family: 'Courier New', monospace; font-size: 1.8em; color: var(--glow-cyan);
            background: linear-gradient(45deg, rgba(0, 255, 255, 0.1), rgba(255, 0, 255, 0.1));
            border: 3px solid var(--glow-cyan); padding: 25px 50px; border-radius: 15px; cursor: pointer;
            text-shadow: 0 0 20px var(--glow-cyan);
            box-shadow: 0 0 40px rgba(0, 255, 255, 0.5), inset 0 0 20px rgba(0, 255, 255, 0.1);
            animation: holoPulse 2s infinite; text-transform: uppercase; letter-spacing: 3px;
        }
        @keyframes holoPulse {
            0%, 100% { opacity: 0.8; box-shadow: 0 0 40px rgba(0, 255, 255, 0.5), inset 0 0 20px rgba(0, 255, 255, 0.1); }
            50% { opacity: 1; box-shadow: 0 0 60px rgba(0, 255, 255, 0.8), inset 0 0 30px rgba(0, 255, 255, 0.2); }
        }

        .main-content {
            display: flex;
            flex-direction: column;
            align-items: center;
            text-align: center;
            z-index: 10;
        }
        .title {
            font-size: 4em; letter-spacing: 12px; font-weight: bold;
            background: linear-gradient(45deg, var(--glow-cyan), var(--glow-magenta), #ffff00, var(--glow-cyan));
            background-size: 300% 300%;
            -webkit-background-clip: text;
            background-clip: text;
            -webkit-text-fill-color: transparent;
            text-shadow: 0 0 30px rgba(0, 255, 255, 0.7);
            animation: glow 3s ease-in-out infinite alternate, gradientShift 6s ease-in-out infinite;
        }
        .subtitle {
            font-size: 1.2em; letter-spacing: 6px; font-weight: bold;
            color: var(--glow-cyan);
            text-shadow: 0 0 15px rgba(0, 255, 255, 0.7);
            margin-bottom: 20px;
            animation: pulse 2s infinite;
        }
        @keyframes glow {
            from { text-shadow: 0 0 30px rgba(0, 255, 255, 0.7), 0 0 50px rgba(255, 0, 255, 0.3); }
            to { text-shadow: 0 0 50px rgba(0, 255, 255, 1), 0 0 80px rgba(255, 0, 255, 0.5); }
        }
        @keyframes gradientShift { 0%, 100% { background-position: 0% 50%; } 50% { background-position: 100% 50%; } }
        @keyframes pulse { 0%, 100% { opacity: 0.7; } 50% { opacity: 1; } }

        /* --- ROBOT FACE --- */
        .robot-face-container {
            width: 300px; /* Larger container for ears */
            height: 250px;
            margin: 20px 0;
            position: relative;
            display: flex;
            align-items: center;
            justify-content: center;
        }
        .robot-face {
            width: 200px; height: 200px;
            position: relative;
            background: radial-gradient(circle, rgba(0, 255, 255, 0.05), rgba(0, 0, 0, 0.5));
            border: 2px solid var(--glow-cyan);
            border-radius: 50%;
            box-shadow: 0 0 30px rgba(0, 255, 255, 0.4), inset 0 0 20px rgba(0, 255, 255, 0.1);
        }
        .robot-ear {
            width: 35px; height: 60px;
            position: absolute;
            top: 50%;
            transform: translateY(-50%);
            background: #222;
            border: 2px solid var(--glow-cyan);
            border-radius: 15px;
            transition: box-shadow 0.3s ease;
        }
        .robot-ear.left { left: 20px; }
        .robot-ear.right { right: 20px; }
        .robot-ear.listening {
            box-shadow: 0 0 25px var(--glow-cyan), 0 0 40px var(--glow-blue);
            animation: earPulse 1s infinite;
        }
        @keyframes earPulse { 0%, 100% { background-color: var(--glow-cyan); } 50% { background-color: var(--glow-blue); } }
        
        .robot-eyes {
            display: flex; justify-content: space-around;
            width: 100px; position: absolute; top: 60px; left: 50%; transform: translateX(-50%);
        }
        .robot-eye {
            width: 30px; height: 30px;
            background: var(--glow-cyan); border-radius: 50%;
            box-shadow: 0 0 15px var(--glow-cyan), inset 0 0 10px white;
            animation: eyeBlink 5s infinite;
        }
        @keyframes eyeBlink { 0%, 95%, 100% { transform: scaleY(1); } 97% { transform: scaleY(0.1); } }

        .robot-mouth {
            position: absolute; bottom: 50px; left: 50%; transform: translateX(-50%);
            width: 80px; height: 10px; background: #111;
            border: 2px solid var(--glow-cyan);
            border-radius: 5px; overflow: hidden;
        }
        .mouth-indicator {
            width: 100%; height: 100%; background: var(--glow-magenta);
            transform: scaleY(0); transform-origin: bottom;
            transition: transform 0.05s ease;
        }
        
        /* --- STATUS RING --- */
        .status-ring-container {
            width: 250px; height: 250px;
            position: absolute;
            top: 50%; left: 50%;
            transform: translate(-50%, -50%);
            z-index: 0;
        }
        .status-ring {
            width: 100%; height: 100%;
            border-radius: 50%;
            border: 3px solid transparent;
            position: absolute;
            transition: all 0.5s ease;
        }
        .status-ring.listening { border-color: var(--glow-cyan); animation: ringPulse 1.5s infinite; }
        .status-ring.thinking { border-color: var(--glow-purple); animation: ringRotate 2s linear infinite; }
        .status-ring.speaking { border-color: var(--glow-magenta); animation: ringPulse 0.8s infinite; }
        .status-ring.user-speaking { border-color: var(--glow-blue); animation: ringPulse 0.5s infinite; }
        @keyframes ringPulse { 0% { transform: scale(0.95); opacity: 0.7; } 70% { transform: scale(1.05); opacity: 1; } 100% { transform: scale(0.95); opacity: 0.7; } }
        @keyframes ringRotate { from { transform: rotate(0deg); } to { transform: rotate(360deg); } }

        #status-text {
            position: fixed; bottom: 30px; left: 50%; transform: translateX(-50%);
            background: rgba(0, 0, 0, 0.7); border: 2px solid var(--glow-cyan);
            border-radius: 30px; padding: 10px 25px; font-size: 1em;
            box-shadow: 0 0 20px rgba(0, 255, 255, 0.5); backdrop-filter: blur(10px);
            z-index: 100; opacity: 0; transition: all 0.3s ease; font-weight: bold;
        }
        #status-text.show { opacity: 1; }
        #status-text.user-speaking { border-color: var(--glow-blue); color: var(--glow-blue); box-shadow: 0 0 20px var(--glow-blue); }
        #status-text.speaking { border-color: var(--glow-magenta); color: var(--glow-magenta); box-shadow: 0 0 20px var(--glow-magenta); }
        #status-text.thinking { border-color: var(--glow-purple); color: var(--glow-purple); box-shadow: 0 0 20px var(--glow-purple); }

        #videoElement { display: none; }
        
        @media (max-width: 768px) {
            .main-content { justify-content: flex-start; padding-top: 10vh; }
            .title { font-size: 2.5em; letter-spacing: 6px; }
            .subtitle { font-size: 1em; }
            .robot-face-container { transform: scale(0.7); margin: -20px 0; }
            #status-text { bottom: 20px; font-size: 0.9em; padding: 8px 15px; }
        }
    </style>
</head>
<body>
    <div id="space-bg" class="parallax-layer"></div>
    <div id="stars" class="parallax-layer"></div>
    <div id="cosmic-dust" class="parallax-layer"></div>
    <div id="planets-container" class="parallax-layer">
        <div class="planet planet-1"></div>
        <div class="planet planet-2"></div>
    </div>

    <div id="start-overlay">
        <button id="start-button">Initialize NEXUS Protocol</button>
    </div>

    <div class="main-content">
        <div class="title">NEXUS 3000</div>
        <div class="subtitle">COSMIC AGI CONSCIOUSNESS</div>
        <div class="robot-face-container">
            <div id="status-ring-1" class="status-ring"></div>
            <div id="status-ring-2" class="status-ring"></div>
            <div class="robot-ear left"></div>
            <div class="robot-face">
                <div class="robot-eyes">
                    <div class="robot-eye"></div>
                    <div class="robot-eye"></div>
                </div>
                <div class="robot-mouth">
                    <div id="mouth-indicator" class="mouth-indicator"></div>
                </div>
            </div>
            <div class="robot-ear right"></div>
        </div>
    </div>
    
    <div id="status-text">Initializing...</div>
    <video id="videoElement" autoplay muted playsinline></video>
    
    <script>
        document.addEventListener('DOMContentLoaded', () => {

            // --- 1. STATE MANAGEMENT & CONFIGURATION ---
            let state = {
                isListening: false,
                isSpeaking: false,
                canListen: true,
                cameraActive: false,
                hasGreeted: false,
            };

            const config = {
                sessionId: Math.random().toString(36).substring(2, 9),
                isIOS: /iPad|iPhone|iPod/.test(navigator.userAgent) && !window.MSStream,
                isAndroid: /Android/.test(navigator.userAgent),
            };

            const SYSTEM_PROMPT = `You are NEXUS 3000, a kind, compassionate best friend... (Your detailed prompt remains here)`;

            // --- 2. AUDIO & SPEECH API SETUP ---
            let recognition = null;
            let audioContext = null;
            let analyser = null;
            let audioSource = null;
            let animationFrameId = null;

            const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

            // --- 3. DOM ELEMENT REFERENCES ---
            const dom = {
                startOverlay: document.getElementById('start-overlay'),
                statusText: document.getElementById('status-text'),
                mouthIndicator: document.getElementById('mouth-indicator'),
                leftEar: document.querySelector('.robot-ear.left'),
                rightEar: document.querySelector('.robot-ear.right'),
                statusRings: [document.getElementById('status-ring-1'), document.getElementById('status-ring-2')],
                parallaxLayers: document.querySelectorAll('.parallax-layer'),
                videoElement: document.getElementById('videoElement'),
            };

            // --- 4. UI UPDATE FUNCTIONS ---
            
            /** Updates the text status indicator at the bottom of the screen. */
            const updateStatusText = (text, statusClass = 'listening', show = true) => {
                dom.statusText.textContent = text;
                dom.statusText.className = 'status-indicator'; // Reset
                if (statusClass) dom.statusText.classList.add(statusClass);
                if (show) dom.statusText.classList.add('show');
            };

            /** Updates the robot's facial features and status rings based on state. */
            const updateRobotState = (newStatus) => {
                // Ears glow only when listening
                const isListening = newStatus === 'listening';
                dom.leftEar.classList.toggle('listening', isListening);
                dom.rightEar.classList.toggle('listening', isListening);
                
                // Rings reflect current status
                dom.statusRings.forEach(ring => ring.className = `status-ring ${newStatus}`);
                dom.statusRings[1].style.animationDelay = newStatus === 'thinking' ? '-1s' : '0s';
            };

            /** Animates the mouth based on audio volume for lip-sync effect. */
            const animateMouth = () => {
                if (!analyser) return;

                const bufferLength = analyser.frequencyBinCount;
                const dataArray = new Uint8Array(bufferLength);
                analyser.getByteFrequencyData(dataArray);

                let sum = 0;
                for (let i = 0; i < bufferLength; i++) {
                    sum += dataArray[i];
                }
                const average = sum / bufferLength;
                
                // Map the average volume (0-255) to a scale (0-1) for the mouth
                const mouthOpenness = Math.min(1, Math.pow(average / 128, 2));
                dom.mouthIndicator.style.transform = `scaleY(${mouthOpenness})`;

                animationFrameId = requestAnimationFrame(animateMouth);
            };

            // --- 5. CORE FUNCTIONALITY ---

            /** Handles mouse movement for the parallax effect. */
            const handleMouseMove = (e) => {
                const { clientX, clientY } = e;
                const { innerWidth, innerHeight } = window;
                const moveX = (clientX - innerWidth / 2) / (innerWidth / 2);
                const moveY = (clientY - innerHeight / 2) / (innerHeight / 2);

                dom.parallaxLayers.forEach((layer, i) => {
                    const depth = (i + 1) * 5; // Adjust depth multiplier
                    layer.style.transform = `translateX(${moveX * depth}px) translateY(${moveY * depth}px)`;
                });
            };

            /** Sets up the Web Speech API for recognition. */
            const setupSpeechRecognition = () => {
                if (!SpeechRecognition) {
                    console.error("Speech recognition not supported.");
                    updateStatusText("❌ Speech not supported", "error", true);
                    return;
                }
                recognition = new SpeechRecognition();
                recognition.continuous = !config.isIOS && !config.isAndroid;
                recognition.interimResults = true;
                recognition.lang = 'en-US';

                recognition.onstart = () => {
                    console.log("Recognition started.");
                    state.isListening = true;
                    updateRobotState('listening');
                    updateStatusText('🎤 Listening...', 'listening', true);
                };

                recognition.onresult = (event) => {
                    if (state.isSpeaking) return;
                    let transcript = '';
                    for (let i = event.resultIndex; i < event.results.length; ++i) {
                        transcript += event.results[i][0].transcript;
                        if (event.results[i].isFinal) {
                            handleUserSpeech(transcript.trim());
                            return;
                        }
                    }
                    if(transcript.trim()) {
                        updateRobotState('user-speaking');
                        updateStatusText('👤 You are speaking...', 'user-speaking', true);
                    }
                };

                recognition.onend = () => {
                    console.log("Recognition ended.");
                    state.isListening = false;
                    if (state.canListen && !state.isSpeaking) {
                        setTimeout(() => recognition.start(), 100);
                    }
                };

                recognition.onerror = (event) => {
                    console.error('Speech recognition error:', event.error);
                    if (event.error === 'no-speech') {
                         updateRobotState('listening'); // Go back to listening
                    } else if (event.error === 'not-allowed') {
                        updateStatusText('❌ Mic access denied', 'error', true);
                    }
                };
            };
            
            /** Processes the final transcript from the user. */
            const handleUserSpeech = async (transcript) => {
                if (!transcript || state.isSpeaking) return;
                console.log(`🎤 USER: "${transcript}"`);

                state.canListen = false;
                state.isSpeaking = true;
                if (recognition) recognition.stop();

                updateRobotState('thinking');
                updateStatusText('🧠 Thinking...', 'thinking', true);

                try {
                    const imageData = state.cameraActive ? captureImage() : null;
                    const responseText = await getAIResponse(transcript, imageData);
                    await speakAIResponse(responseText);
                } catch (error) {
                    console.error("Error in speech handling pipeline:", error);
                    await speakAIResponse("I seem to have encountered a glitch in my system.");
                } finally {
                    state.isSpeaking = false;
                    state.canListen = true;
                    if (recognition && !state.isListening) {
                        recognition.start();
                    }
                }
            };
            
            /** Fetches a response from the AI backend. (Placeholder) */
            const getAIResponse = async (userMessage, imageData = null) => {
                // This is where you would call your backend API.
                // For demonstration, it returns a canned response.
                console.log("Sending to AI backend (placeholder)...");
                const requestData = { 
                    message: `${SYSTEM_PROMPT}\n\nUser: "${userMessage}"`,
                    session_id: config.sessionId,
                    image_data: imageData ? imageData.split(',')[1] : null
                };
                
                // MOCK API CALL
                return new Promise(resolve => {
                    setTimeout(() => {
                        resolve(`You said "${userMessage}". I am processing this information within my cosmic consciousness.`);
                    }, 1500);
                });
                
                /*
                // REAL API CALL EXAMPLE
                const response = await fetch('/api/chat', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify(requestData)
                });
                if (!response.ok) throw new Error('AI API Error');
                const data = await response.json();
                return data.response;
                */
            };

            /** Speaks the AI's response using TTS and lip-sync. */
            const speakAIResponse = async (text) => {
                return new Promise(async (resolve) => {
                    updateRobotState('speaking');
                    updateStatusText('🤖 Speaking...', 'speaking', true);

                    try {
                        // MOCK TTS API CALL
                        const audioBlob = await new Promise(res => setTimeout(() => {
                            // This part would be replaced by a fetch to your TTS service
                            const utterance = new SpeechSynthesisUtterance(text);
                            // This is a complex workaround to get a blob from browser TTS.
                            // A real TTS API would just return the audio file directly.
                            // We will skip the complexity and resolve when speech ends.
                            utterance.onend = () => resolve(null); // No blob for fallback
                            speechSynthesis.speak(utterance);
                        }, 100));
                        
                        // Firing this for demonstration purposes with fallback
                        handleAudioPlayback(null, text, resolve);

                    } catch (error) {
                        console.error("TTS API failed, using fallback.", error);
                        handleAudioPlayback(null, text, resolve); // Use fallback
                    }
                });
            };

            /** Handles audio playback, lip-sync, and fallback TTS. */
            const handleAudioPlayback = (audioBlob, text, onEndCallback) => {
                if (audioBlob && audioContext) {
                    // --- Preferred Path: Web Audio API with Lip Sync ---
                    const reader = new FileReader();
                    reader.onload = (e) => {
                        audioContext.decodeAudioData(e.target.result, (buffer) => {
                            if (audioSource) audioSource.disconnect();
                            audioSource = audioContext.createBufferSource();
                            audioSource.buffer = buffer;
                            audioSource.connect(analyser);
                            analyser.connect(audioContext.destination);
                            audioSource.start(0);
                            animateMouth();
                            audioSource.onended = () => onPlaybackEnd(onEndCallback);
                        });
                    };
                    reader.readAsArrayBuffer(audioBlob);
                } else {
                    // --- Fallback Path: Browser SpeechSynthesis ---
                    if ('speechSynthesis' in window) {
                        const utterance = new SpeechSynthesisUtterance(text);
                        // No reliable way to get audio data for lip-sync from this API,
                        // so we just use a generic mouth animation.
                        utterance.onstart = () => dom.mouthIndicator.style.transform = 'scaleY(0.7)';
                        utterance.onend = () => onPlaybackEnd(onEndCallback);
                        speechSynthesis.speak(utterance);
                    } else {
                        onPlaybackEnd(onEndCallback);
                    }
                }
            };
            
            /** Cleanup logic after audio playback finishes. */
            const onPlaybackEnd = (callback) => {
                if (animationFrameId) cancelAnimationFrame(animationFrameId);
                dom.mouthIndicator.style.transform = 'scaleY(0)';
                if (audioSource) audioSource.disconnect();
                callback();
            };

            // --- 6. INITIALIZATION & EVENT LISTENERS ---

            /** Initializes media devices (camera/mic). */
            const initializeMedia = async () => {
                try {
                    console.log("Requesting media permissions...");
                    const stream = await navigator.mediaDevices.getUserMedia({
                        video: { width: { ideal: 640 }, height: { ideal: 480 }, facingMode: 'user' },
                        audio: { echoCancellation: true, noiseSuppression: true }
                    });
                    dom.videoElement.srcObject = stream;
                    state.cameraActive = true;
                    console.log("✅ Media devices initialized.");
                } catch (error) {
                    console.warn("Camera failed or was denied. Proceeding with audio only.", error.name);
                    state.cameraActive = false;
                    try {
                        const audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });
                        console.log("✅ Microphone initialized.");
                    } catch (audioError) {
                        console.error("❌ Microphone access also denied.", audioError.name);
                        updateStatusText("❌ Mic access required", "error", true);
                    }
                }
            };
            
            /** Captures a single frame from the video feed. */
            const captureImage = () => {
                const canvas = document.createElement('canvas');
                canvas.width = dom.videoElement.videoWidth;
                canvas.height = dom.videoElement.videoHeight;
                const ctx = canvas.getContext('2d');
                ctx.drawImage(dom.videoElement, 0, 0, canvas.width, canvas.height);
                return canvas.toDataURL('image/jpeg', 0.7);
            };

            /** The main function to start the application. */
            const startNexus = async () => {
                console.log("🚀 NEXUS 3000 Protocol Initiated");
                dom.startOverlay.style.opacity = '0';
                setTimeout(() => dom.startOverlay.style.display = 'none', 500);
                
                // Initialize AudioContext after user interaction
                if (!audioContext) {
                    audioContext = new (window.AudioContext || window.webkitAudioContext)();
                    analyser = audioContext.createAnalyser();
                    analyser.fftSize = 256;
                }

                await initializeMedia();
                setupSpeechRecognition();
                
                document.body.addEventListener('mousemove', handleMouseMove);

                setTimeout(async () => {
                    if (!state.hasGreeted) {
                        state.hasGreeted = true;
                        handleUserSpeech("Introduce yourself."); // Initial greeting
                    }
                }, 1000);
            };

            // Add the primary event listener to the start button
            dom.startOverlay.addEventListener('click', startNexus, { once: true });
        });
    </script>
</body>
</html>
